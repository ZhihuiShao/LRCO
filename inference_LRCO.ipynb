{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.distributions import Categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization,Flatten,Dense,Input,Activation,Dropout,Reshape,Softmax, LSTM\n",
    "from tensorflow.keras.layers import Conv2D, Bidirectional, GRU\n",
    "from tensorflow.keras import Sequential, models, optimizers, layers, losses, metrics, regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import scipy.optimize as sop\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "from tensorflow.contrib.layers import xavier_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "global_user = 20\n",
    "global_x_min = 0\n",
    "global_x_max = 1\n",
    "global_eta = 0.02\n",
    "global_epsilon = 0.71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maximizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the lam=5 maxi\n",
    "lamb = 1\n",
    "def my_max_loss(y_true, y_pred): #[None,300],None[150]\n",
    "    y_true_x = y_true[:,:global_user]+y_pred\n",
    "    y_true_a = y_true[:,global_user:]\n",
    "    y_true_x = tf.reshape(y_true_x,[-1,4,5])\n",
    "    y_true_a = tf.reshape(y_true_a,[-1,4,5])\n",
    "    xa = 1-(y_true_x)*y_true_a #shape=(None,4,5)\n",
    "    loss = K.prod(1-K.prod(xa,axis=-1),axis=-1,keepdims=True) # shape=(None,1)\n",
    "    loss = loss-global_eta*K.sum(K.sum(y_true_a,axis=-1),axis=-1,keepdims=True) # shape(None,1)\n",
    "    bounds = K.sqrt(K.sum(K.square(y_pred),axis=-1,keepdims=True))-global_epsilon # shape: (None,1)\n",
    "    bounds1 = K.sum(K.sum(K.maximum(1.0,y_true_x)-1,axis=-1),axis=-1,keepdims=True)\n",
    "    bounds2 = K.sum(K.sum(K.maximum(0.0,-1*y_true_x),axis=-1),axis=-1,keepdims=True)\n",
    "    loss2 = K.maximum(bounds,0) # shape: (None,1)\n",
    "    return loss+lamb*loss2+100*(bounds1+bounds2)\n",
    "# maximize model\n",
    "def get_maxi_model(hiddens,model_name):\n",
    "    \"\"\"maxi models \"\"\"\n",
    "    xa = Input(shape=(global_user*2,),name='input')\n",
    "    a = xa[:,global_user:]    \n",
    "    h = Dense(hiddens[0],activation=None,name='hidden_0')(xa)\n",
    "#     h = BatchNormalization(name='BN_0')(h)\n",
    "    h = Activation(activation='relu',name='Act_0')(h)\n",
    "    for i,h_num in enumerate(hiddens[1:]):\n",
    "        h = Dense(h_num,activation='relu',name='hidden_{}'.format(i+1))(h)\n",
    "#         h = BatchNormalization(name='BN_{}'.format(i+1))(h)\n",
    "        h = Activation(activation='relu',name='Act_{}'.format(i+1))(h)\n",
    "    out = Dense(units=global_user,activation='tanh',name='out')(h)#noise\n",
    "    out = 0.71*out*a\n",
    "    model = models.Model(inputs=xa,outputs=out,name=model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zhihui\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x16bb7142f88>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_nodes = [200,200]#[200,200]\n",
    "layer_num = 2\n",
    "file_path = '../checkpoints_pretrain/worst_case_model_layer'\n",
    "model_1 = get_maxi_model(hiddens=hidden_nodes,model_name='maxi')\n",
    "model_1.compile(loss=my_max_loss,optimizer=optimizers.Adam(learning_rate=1e-5))\n",
    "model_1.load_weights(file_path+'{}_200_lam1'.format(layer_num))\n",
    "model_2 = get_maxi_model(hiddens=hidden_nodes,model_name='maxi')\n",
    "model_2.compile(loss=my_max_loss,optimizer=optimizers.Adam(learning_rate=1e-5))\n",
    "model_2.load_weights(file_path+'{}_200_lam2'.format(layer_num))\n",
    "\n",
    "model_5 = get_maxi_model(hiddens=hidden_nodes,model_name='maxi')\n",
    "model_5.compile(loss=my_max_loss,optimizer=optimizers.Adam(learning_rate=1e-5))\n",
    "model_5.load_weights(file_path+'{}_200_lam5'.format(layer_num))\n",
    "\n",
    "model_10 = get_maxi_model(hiddens=hidden_nodes,model_name='maxi')\n",
    "model_10.compile(loss=my_max_loss,optimizer=optimizers.Adam(learning_rate=1e-5))\n",
    "model_10.load_weights(file_path+'{}_200_lam8'.format(layer_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_worst_case(x_val_pair):\n",
    "    x_val = x_val_pair[:,:global_user]\n",
    "    action_val = x_val_pair[:,global_user:]\n",
    "    \n",
    "    # get worse\n",
    "    x_val_worse_1 = model_1.predict(x_val_pair).reshape([-1,global_user])\n",
    "    x_val_worse_2 = model_2.predict(x_val_pair).reshape([-1,global_user])\n",
    "    x_val_worse_5 = model_5.predict(x_val_pair).reshape([-1,global_user])\n",
    "    x_val_worse_10 = model_10.predict(x_val_pair).reshape([-1,global_user])\n",
    "\n",
    "    dx_1 = np.sqrt(np.sum(np.square(x_val_worse_1),axis=-1,keepdims=True))\n",
    "    dx_2 = np.sqrt(np.sum(np.square(x_val_worse_2),axis=-1,keepdims=True))\n",
    "    dx_5 = np.sqrt(np.sum(np.square(x_val_worse_5),axis=-1,keepdims=True))\n",
    "    dx_10 = np.sqrt(np.sum(np.square(x_val_worse_10),axis=-1,keepdims=True))\n",
    "    \n",
    "    ratio_1 = np.maximum(dx_1/global_epsilon,1)\n",
    "    ratio_2 = np.maximum(dx_2/global_epsilon,1)\n",
    "    ratio_5 = np.maximum(dx_5/global_epsilon,1)\n",
    "    ratio_10 = np.maximum(dx_10/global_epsilon,1)\n",
    "\n",
    "    x_val_worse_add_1 = x_val_pair[:,:global_user]+x_val_worse_1/ratio_1\n",
    "    x_val_worse_add_2 = x_val_pair[:,:global_user]+x_val_worse_2/ratio_2\n",
    "    x_val_worse_add_5 = x_val_pair[:,:global_user]+x_val_worse_5/ratio_5\n",
    "    x_val_worse_add_10 = x_val_pair[:,:global_user]+x_val_worse_10/ratio_10\n",
    "    \n",
    "    x_val_worse_add_1 = np.minimum(np.maximum(x_val_worse_add_1,global_x_min),global_x_max)\n",
    "    x_val_worse_add_2 = np.minimum(np.maximum(x_val_worse_add_2,global_x_min),global_x_max)\n",
    "    x_val_worse_add_5 = np.minimum(np.maximum(x_val_worse_add_5,global_x_min),global_x_max)\n",
    "    x_val_worse_add_10 = np.minimum(np.maximum(x_val_worse_add_10,global_x_min),global_x_max)\n",
    "    \n",
    "    obj_1 = obj_fun(x_val_worse_add_1,action_val).reshape([-1,1])\n",
    "    obj_2 = obj_fun(x_val_worse_add_2,action_val).reshape([-1,1])\n",
    "    obj_5 = obj_fun(x_val_worse_add_5,action_val).reshape([-1,1])\n",
    "    obj_10 = obj_fun(x_val_worse_add_10,action_val).reshape([-1,1])\n",
    "    \n",
    "    obj_ensemble = np.hstack((obj_1,obj_2,obj_5,obj_10))\n",
    "    obj_worse = np.min(obj_ensemble,axis=-1,keepdims=True)\n",
    "    return obj_worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robust_performance(x_val,actions): # return [None,1]\n",
    "    my_p = calcuate_worst_case(np.hstack((x_val.reshape([-1,global_user]),\n",
    "                                          actions.reshape([-1,global_user]))))\n",
    "    return my_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun(x,a): # input is numpy [None,20]\n",
    "    \"\"\" object function to minimize \"\"\"\n",
    "    x,a = x.reshape([-1,global_user]),a.reshape([-1,global_user])\n",
    "    res = 1-x*a  #[None,20]\n",
    "    res = res.reshape([-1,4,5]) #[None,4,5]\n",
    "    res = 1-np.prod(res,axis=-1) #[None,4]\n",
    "    res = np.prod(res,axis=-1) #[None]\n",
    "    res = res-global_eta*np.sum(a,axis=-1) #[None,]\n",
    "    return res #[None,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = [50,50]\n",
    "global_T = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-7e1c25275174>:21: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\Zhihui\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\categorical.py:242: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\Zhihui\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "WARNING:tensorflow:From <ipython-input-11-7e1c25275174>:27: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Zhihui\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# def the actor graph # begin by result all\n",
    "tf.random.set_random_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "# define the graph\n",
    "x_input = tf.placeholder(dtype=tf.float32,shape=[None,global_user,],name='x_input')\n",
    "a_value = tf.placeholder(dtype=tf.float32,shape=[None,1],name='value_input')\n",
    "with tf.variable_scope(\"actor\"):\n",
    "    h = Reshape(target_shape=(global_user,1,1),name='in_reshape')(x_input)\n",
    "    embed = Conv2D(filters=hidden_size[0],kernel_size=(1,1),strides=(1,1),activation='relu',name='embed')(h)\n",
    "    embed = Reshape(target_shape=(global_user,hidden_size[0]),name='in_reshape')(embed)\n",
    "    h1 = LSTM(hidden_size[1], return_sequences=True, return_state=False,name='LSTM1')(embed)\n",
    "    logit = LSTM(1, return_sequences=True, return_state=False,name='logit')(h1) #[None,10,1]\n",
    "    logit = Reshape(target_shape=(global_user,),name='out_reshape')(logit)#[None,10]\n",
    "    logit = logit/global_T\n",
    "    out_prob = Activation(activation='sigmoid',name='output')(logit)#[None,10]\n",
    "    out_prob = Reshape(target_shape=(global_user,1))(out_prob)\n",
    "    out_prob_r = 1-out_prob\n",
    "    out_prob_final = tf.concat((out_prob,out_prob_r),axis=-1)#[None,10,2]\n",
    "    # obtain probability by sampling and get the onehot_p as the final action\n",
    "    dist = Categorical(probs=out_prob_final)\n",
    "    action = dist.sample()\n",
    "    action = tf.cast(action,tf.int32) #shape=(None,10)\n",
    "    \n",
    "    select_action = tf.placeholder(dtype=tf.int32,shape=[None,global_user],name='select_action')\n",
    "    log_p = dist.log_prob(select_action) #shape=(None,15) corresponding to the log of position\n",
    "    log_p_sum = tf.reduce_sum(log_p,axis=-1,keep_dims=True)\n",
    "    reward = tf.placeholder(dtype=tf.float32,shape=[None,1],name='reward')\n",
    "    \n",
    "    # get the loss and optimizer\n",
    "    delta_reward = reward-a_value # shape(None,1)\n",
    "    actor_loss = tf.reduce_mean(-1*delta_reward*log_p_sum,axis=0) # shape(0,)\n",
    "    \n",
    "    # set for lr decay\n",
    "    global_step1 = tf.Variable(0, trainable=False, name=\"global_step1\")\n",
    "    lr1_start = 2e-3 # initial learning rate\n",
    "    lr1_decay_rate = 0.9 # learning rate decay rate\n",
    "    lr1_decay_step = 20*10 # learning rate decay step\n",
    "    lr1 = tf.train.exponential_decay(lr1_start,global_step1,lr1_decay_step,\n",
    "                                     lr1_decay_rate, staircase=False, name=\"learning_rate1\")\n",
    "    \n",
    "    actor_opt = tf.train.AdamOptimizer(learning_rate=lr1,beta1=0.9,beta2=0.99, epsilon=0.0000001)   \n",
    "    gvs = actor_opt.compute_gradients(actor_loss)\n",
    "    capped_gvs = [(tf.clip_by_norm(grad, 1.), var) for grad, var in gvs if grad is not None]\n",
    "    actor_train = actor_opt.apply_gradients(capped_gvs,global_step=global_step1) #actor_train = actor_opt.minimize(actor_loss)\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zhihui\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./LSTM/model_2layer_50.ckpt-100\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver.restore(sess,'./LSTM/model_2layer_50.ckpt-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "names='test'\n",
    "x_test = sio.loadmat('../data/x_{}.mat'.format(names))['p_linear']\n",
    "x_test_true = sio.loadmat('../data/x_{}.mat'.format(names))['p_true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.11753886144921967=>10 : 0.3957132492839687=>20 : 0.4274355512254422=>30 : 0.4425243899484962=>40 : 0.450844684894619=>50 : 0.45683894033717054=>60 : 0.4612187858845544=>70 : 0.4646495282290423=>80 : 0.46713641801004796=>90 : 0.46937719254438826=>100 : 0.4715509199798075=>110 : 0.4733228404219489=>120 : 0.4747269561380053=>130 : 0.4761071954945386=>140 : 0.4771654462423723=>150 : 0.4782763868684124=>160 : 0.4793438234063182=>170 : 0.48034918506253327=>180 : 0.481240533375604=>190 : 0.4819254866732026=>200 : 0.48267087899471134=>210 : 0.48345266836131906=>220 : 0.48408257780891834=>230 : 0.4845788173652314=>240 : 0.48519312411935833=>250 : 0.48568157431686276=>260 : 0.48625121145953387=>270 : 0.4868080914449705=>280 : 0.4872822281321744=>290 : 0.4877400417194889=>300 : 0.48812704583827327=>310 : 0.4885789490182038=>320 : 0.4889706877680485=>330 : 0.4893370697944931=>340 : 0.48968505738609047=>350 : 0.49000125420622326=>360 : 0.4903104019086317=>370 : 0.49069466011679963=>380 : 0.4910080900886255=>390 : 0.49131826683776897=>400 : 0.4916208227968551=>410 : 0.4919083267703324=>420 : 0.4921700293440429=>430 : 0.4924242609751354=>440 : 0.492628390763515=>450 : 0.4929125226742321=>460 : 0.493158895509194=>470 : 0.49341090717800024=>480 : 0.49362511510215035=>490 : 0.49384238974755207=>500 : 0.49405699827844307=>510 : 0.4943110227629064=>520 : 0.49448233649507656=>530 : 0.4946736206197409=>540 : 0.4948242894907972=>550 : 0.49499565235963694=>560 : 0.4952211894971855=>570 : 0.49544007701378834=>580 : 0.49562606249601593=>590 : 0.49578463496341485=>600 : 0.4959487161898921=>610 : 0.49612329459456106=>620 : 0.49630099393764665=>630 : 0.49644635469479553=>640 : 0.49660436134829633=>650 : 0.49674833272884117=>660 : 0.4968999884544714=>670 : 0.4970733128524514=>680 : 0.497248457208927=>690 : 0.4973777507875201=>700 : 0.4975103436622246=>710 : 0.4976718792351576=>720 : 0.49780080176370994=>730 : 0.49788696667850174=>740 : 0.49800349992951143=>750 : 0.4981162951481205=>760 : 0.49826234491753274=>770 : 0.49838422583022385=>780 : 0.4985339775501438=>790 : 0.49869838636921104=>800 : 0.4988021948987415=>810 : 0.4988849063578629=>820 : 0.4990131889298076=>830 : 0.4991433528999087=>840 : 0.49922809244659183=>850 : 0.49929479018218115=>860 : 0.49943725854518345=>870 : 0.4995492118481132=>880 : 0.49964914382987125=>890 : 0.49975849969537495=>900 : 0.4998549577250493=>910 : 0.4999637170606851=>920 : 0.5000616183923982=>930 : 0.5001380255973787=>940 : 0.5002391402347415=>950 : 0.5003224525093349=>960 : 0.5004162380879257=>970 : 0.5004867894543861=>980 : 0.5005698305185815=>990 : 0.5006620422528817=>"
     ]
    }
   ],
   "source": [
    "# cur_reward,cur_onehot_action = sess.run([reward,onehot_action],{x_input:x_test})\n",
    "all_reward = None\n",
    "all_action = None\n",
    "mean_reward = []\n",
    "for i in range(1000):\n",
    "    onepass_action = sess.run(action,{x_input:x_test}) #[None,5]\n",
    "    onepass_reward = get_robust_performance(x_test,onepass_action).reshape([-1])\n",
    "    \n",
    "    if all_reward is None:\n",
    "        all_reward = onepass_reward\n",
    "        all_action = onepass_action\n",
    "    else:\n",
    "        mask = 1*(all_reward>=onepass_reward)\n",
    "        all_reward = mask*all_reward+(1-mask)*onepass_reward\n",
    "        mask = mask.reshape([-1,1])\n",
    "        all_action = mask*all_action+(1-mask)*onepass_action\n",
    "    mean_reward.append(np.mean(all_reward))\n",
    "    \n",
    "    if i%10==0:\n",
    "        print(i,':',mean_reward[-1],end='=>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500741931717777 0.6434909437478815\n"
     ]
    }
   ],
   "source": [
    "ensemble_reward = all_reward\n",
    "ensemble_action = all_action\n",
    "print(np.mean(ensemble_reward),np.mean(obj_fun(x_test,ensemble_action,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('../result/robustpolicyG_test_2layer_50_sample1000.mat',\n",
    "            {'action':ensemble_action,'performance':ensemble_reward,\n",
    "             'mean_reward':mean_reward})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
